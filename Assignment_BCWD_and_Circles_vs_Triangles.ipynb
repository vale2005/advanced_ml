{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment -- BCWD and Circles vs Triangles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9_5MHu9SKty",
        "colab_type": "text"
      },
      "source": [
        "# Assigment\n",
        "\n",
        "The following notebook contains the base architecture for the assignment.\n",
        "\n",
        "The task is to complete the missing parts, explore the datasets and build two simple binary classifier, one which consists only of fully connected layers and one which also incorporates convolutional and max-pooling layers.\n",
        "\n",
        "Only numpy is allowed to implement the classes! (Matplotlib and other modules can be used for visualization and data exploration.)\n",
        "\n",
        "**Due date:** 2019 december 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG1JXnLcUV3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9d-1x2cZzYa",
        "colab_type": "text"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBs6_-e8UWjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optimizer:\n",
        "  def update(self, param, grad):\n",
        "    pass\n",
        "  \n",
        "  def __call__(self, param, grad):\n",
        "    self.update(param, grad)\n",
        "  \n",
        "  \n",
        "class SGD(Optimizer):\n",
        "  def __init__(self, learning_rate):\n",
        "    self.learning_rate = learning_rate\n",
        "  \n",
        "  def update(self, param, grad):\n",
        "    '''Gradient Descent Update\n",
        "    This function updates the given 'param' using the 'grad' (gradients). \n",
        "    Note #1: Use the learning_rate. \n",
        "    Note #2: There are no return values.\n",
        "    \n",
        "    :param param: Parameters of the layer.\n",
        "    :param grad: Corresponding gradients.\n",
        "    '''\n",
        "    # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DksZNjk3Z9s8",
        "colab_type": "text"
      },
      "source": [
        "# Weight Initializers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sVP-7TgVsmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WeigthInitializer:\n",
        "  def initialize(self, size):\n",
        "    return np.ones(size, dtype=np.float)\n",
        "  \n",
        "  def __call__(self, size):\n",
        "    return self.initialize(size)\n",
        "  \n",
        "\n",
        "class RandomInitializer(WeigthInitializer):\n",
        "  def __init__(self, shift=-0.5, scale=0.2):\n",
        "    self.shift = shift\n",
        "    self.scale = scale\n",
        "    \n",
        "  def initialize(self, size):\n",
        "    '''Random number initializer\n",
        "    Note #1: 'self.scale' specifies the range of the values and with 'self.shift' they can be shifted.\n",
        "    Note #2: By default (with scale=0.2 and shift=-0.5) it should return a matrix which contains random values between -0.1 and 0.1. \n",
        "    Note #3: Use the np.random modul!\n",
        "\n",
        "    :param size: Dimensions of the matrix.\n",
        "    :returns: A matrix of random numbers with dimensions specified by 'size'. \n",
        "    '''\n",
        "    return None  # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIc59gXDaRsK",
        "colab_type": "text"
      },
      "source": [
        "# Function class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwWfbE61aHeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Function:\n",
        "  def forward(self, input):\n",
        "    return None  \n",
        "  \n",
        "  def __call__(self, input):\n",
        "    return self.forward(input)\n",
        "  \n",
        "  def backward(self, grads):\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HidyOxGGZyvV",
        "colab_type": "text"
      },
      "source": [
        "## Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1B1lLxTaLrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activation(Function):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  \n",
        "class Linear(Activation):\n",
        "  def forward(self, z):\n",
        "    return z.astype(np.float)\n",
        "\n",
        "  def backward(self, z):\n",
        "    return np.ones_like(z, dtype=np.float)\n",
        "\n",
        "\n",
        "class Relu(Activation):\n",
        "  def forward(self, z):\n",
        "    '''Forward pass of the Rectified Linear Unit activation function.\n",
        "\n",
        "    :param z: Input tensor.\n",
        "    :returns: ReLU(z), see the lecture notes for the definition. \n",
        "    '''\n",
        "    return None  # TODO\n",
        "\n",
        "  def backward(self, z):\n",
        "    '''Backward pass of the Rectified Linear Unit activation function.\n",
        "\n",
        "    :param z: Input tensor.\n",
        "    :returns: ReLU'(z), see the lecture notes for the definition. \n",
        "    '''\n",
        "    return None  # TODO\n",
        "\n",
        "  \n",
        "class Sigmoid(Activation):\n",
        "  def forward(self, z):\n",
        "    '''Forward pass of the Sigmoid activation function.\n",
        "\n",
        "    :param z: Input tensor.\n",
        "    :returns: sigmoid(z), see the lecture notes for the definition. \n",
        "    '''\n",
        "    return None  # TODO\n",
        "  \n",
        "  def backward(self, z):\n",
        "    '''Backward pass of the Sigmoid activation function.\n",
        "\n",
        "    :param z: Input tensor.\n",
        "    :returns: sigmoid'(z), see the lecture notes for the definition. \n",
        "    '''\n",
        "    return None  # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NDdM8JNbqJN",
        "colab_type": "text"
      },
      "source": [
        "## Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHY8E2ZPbuLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Loss(Function):\n",
        "  def forward(self, y_true, y_pred):\n",
        "    return None\n",
        "  \n",
        "  def __call__(self, y_true, y_pred):\n",
        "    return self.forward(y_true, y_pred)\n",
        "  \n",
        "  def backward(self, y_true, y_pred):\n",
        "    return None\n",
        "  \n",
        "  \n",
        "class BinaryCrossentropy(Loss):\n",
        "  def forward(self, y_true, y_pred):\n",
        "    '''Forward pass of the Binary Crossentropy loss.\n",
        "    Note: Both 'y_true' and 'y_pred' contains a batch of labels => y_true.shape == y_pred.shape == <batch size> x 1 \n",
        "\n",
        "    :param y_true: Ground truth labels.\n",
        "    :param y_pred: Predicted labels.\n",
        "    :returns: Binary crossentropy loss, see the lecture notes for the definition. \n",
        "    '''\n",
        "    return None  # TODO\n",
        "  \n",
        "  def backward(self, y_true, y_pred):\n",
        "    '''Backward pass of the Binary Crossentropy loss.\n",
        "    Note #1: The gradient should have the same shape as y_pred (<batch size> x 1) \n",
        "    Note #2: Keep in mind that the derivative of the loss in the lecture notes is for a logistic regression model. \n",
        "    Note #3: Here, you do not need to derive respect to the weights! \n",
        "\n",
        "    :param y_true: Ground truth labels.\n",
        "    :param y_pred: Predicted labels.\n",
        "    :returns: Derivative of the binary crossentropy loss, see the lecture notes for the \"definition\". \n",
        "    '''\n",
        "    return None  # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRcHiINgd9t-",
        "colab_type": "text"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkbVSEU5eHYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer(Function):\n",
        "  def __init__(self, activation, optimizer=None, weight_init=RandomInitializer(), *args, **kwargs):\n",
        "    self.activation = activation\n",
        "    self.optimizer = optimizer\n",
        "    self.weight_init = weight_init\n",
        "  \n",
        "  def _forward(self, x):\n",
        "    return None\n",
        "  \n",
        "  def forward(self, X):\n",
        "    self.X = X\n",
        "    self.Z = self._forward(X)\n",
        "    self.h = self.activation(self.Z)\n",
        "    return self.h\n",
        "  \n",
        "  def _backward(self, dZ):\n",
        "    return None, None\n",
        "  \n",
        "  def backward(self, dh):\n",
        "    dZ = dh * self.activation.backward(self.Z)\n",
        "    self.dX, self.grads = self._backward(dZ)\n",
        "    self._update_weights()\n",
        "    return self.dX\n",
        "  \n",
        "  def _update_weights(self):\n",
        "    assert len(self.params) == len(self.grads)\n",
        "    for idx in range(len(self.params)):\n",
        "      self.optimizer(self.params[idx], self.grads[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmpzs9KweKfl",
        "colab_type": "text"
      },
      "source": [
        "### Fully-connected (dense) layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2IN9LRReScS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense(Layer):\n",
        "  def __init__(self, size, *args, **kwargs):\n",
        "    super(Dense, self).__init__(*args, **kwargs)\n",
        "    self.W = self.weight_init(size)\n",
        "    self.b = self.weight_init((1, size[1]))\n",
        "    self.params = [self.W, self.b]\n",
        "    \n",
        "  def _forward(self, X):\n",
        "    '''Forward pass of the dense layer.\n",
        "    Note #1: Use self.W and self.b\n",
        "    Note #2: Input times weight add a bias ==> activate is already taken care of! (see self.forward())\n",
        "\n",
        "    :param X: Input matrix\n",
        "    :returns: Linear combination, see the lecture notes for the definition.\n",
        "    '''\n",
        "    return None  # TODO\n",
        "\n",
        "  def _backward(self, dZ):\n",
        "    '''Backward pass of the dense layer.\n",
        "    Note: Use self.X\n",
        "\n",
        "    :param dZ: Gradient of the subsequent layer.\n",
        "    :returns: A pair (dX and [dW, db]) which contains the partial derivatives respect to the input and to the parameters (W and b). See the lecture notes for the \"definition\".\n",
        "    '''\n",
        "    dW = None  #TODO\n",
        "    db = None  #TODO\n",
        "    dX = None  #TODO\n",
        "    return dX, [dW, db]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoocZjuRgkbC",
        "colab_type": "text"
      },
      "source": [
        "### Flatten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2M8ROl3gmtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(Layer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(Flatten, self).__init__(activation=Linear(), *args, **kwargs)\n",
        "  \n",
        "  def _forward(self, X):\n",
        "    return X.reshape((len(X), -1))\n",
        "\n",
        "  def _backward(self, dZ):\n",
        "    return dZ.reshape(self.X.shape), []\n",
        "  \n",
        "  def _update_weights(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M_Dfks0gqkS",
        "colab_type": "text"
      },
      "source": [
        "### Max pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUMqKR7egwSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Maxpool2d(Layer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(Maxpool2d, self).__init__(activation=Linear(), *args, **kwargs)\n",
        "  \n",
        "  def _forward(self, X):\n",
        "    '''Forward pass of the max pooling layer.\n",
        "\n",
        "    :param X: Input matrix\n",
        "    :returns: Matrix (<batch_size> x <height>//2 x <width>//2 x <n_channels>) after max pooling, see the lecture notes for the definition.\n",
        "    '''    \n",
        "    self.mask = None  # TODO save the mask for later (_backward) use.\n",
        "    return None  # TODO\n",
        "\n",
        "  def _backward(self, dZ):\n",
        "    '''Backward pass of the max pooling layer.\n",
        "    Note: Use self.mask too.\n",
        "\n",
        "    :param dZ: Gradient of the subsequent layer.\n",
        "    :returns: A pair (dX and []) which contains the partial derivative respect to the input and an empty list. See the lecture notes for the \"definition\".\n",
        "    '''\n",
        "    dX = None  # TODO\n",
        "    return dX, []\n",
        "  \n",
        "  def _update_weights(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZptff2Age3L",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q0AXVEngiDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv2d(Layer):\n",
        "  def __init__(self, kernel_size, n_channels, n_kernels, pad, use_fast=False, *args, **kwargs):    \n",
        "    super(Conv2d, self).__init__(*args, **kwargs)\n",
        "    self.W = self.weight_init((kernel_size, kernel_size, n_channels, n_kernels))\n",
        "    self.b = self.weight_init((1, 1, 1, n_kernels))\n",
        "    self.params = [self.W, self.b]\n",
        "    self.pad = pad\n",
        "    self.use_fast = use_fast\n",
        "  \n",
        "  def _convolution_fast(self, Y):\n",
        "    '''Optimized version of the convolution operation (Optional).\n",
        "    Note #1: Use self.X, self.X_padded, self.W\n",
        "    Note #2: There are no return values.\n",
        "    Note #3: It's an optional task.\n",
        "    \n",
        "    :param Y: Destination (output) matrix (image), see the lecture notes for the \"definition\". \n",
        "    '''    \n",
        "    pass  # TODO optional\n",
        "    \n",
        "  def _convolution_slow(self, Y):\n",
        "    '''Naive version (with a bunch of for loops) of the convolution operation.\n",
        "    Note #1: Use self.X, self.X_padded, self.W\n",
        "    Note #2: There are no return values.\n",
        "    Note #3: Both convolution and cross-correlation is acceptable.\n",
        "    \n",
        "    :param Y: Destination (output) matrix (image), see the lecture notes for the \"definition\". \n",
        "    '''    \n",
        "    pass  # TODO\n",
        "\n",
        "  def _forward(self, X):\n",
        "    y_height = X.shape[1] - self.W.shape[0] + 2*self.pad + 1\n",
        "    y_width = X.shape[2] - self.W.shape[1] + 2*self.pad + 1\n",
        "    Y = np.zeros((X.shape[0], y_height, y_width, self.W.shape[3]), dtype=np.float16)\n",
        "    \n",
        "    if 0 < self.pad:\n",
        "      X_padded = # TODO pad the input\n",
        "    else:\n",
        "      X_padded = X\n",
        "    self.X_padded = X_padded\n",
        "    \n",
        "    if self.use_fast:\n",
        "      self._convolution_fast(Y)\n",
        "    else:\n",
        "      self._convolution_slow(Y)\n",
        "    \n",
        "    Y += self.b\n",
        "    return Y\n",
        "  \n",
        "  def _backward_fast(self, dZ):\n",
        "    '''Optimized version of the backward pass (Optional).\n",
        "    Note #1: Use self.X, self.X_padded, self.W\n",
        "    Note #2: It's an optional task.\n",
        "    \n",
        "    :param dZ: Gradient of the subsequent layer.\n",
        "    :returns: A pair (dX, dW and db) which contains the partial derivatives respect to the input and to the parameters (W and b). See the lecture notes for the \"definition\".\n",
        "    '''    \n",
        "    db = None  # None optional\n",
        "    dW = None  # None optional\n",
        "    dX = None  # None optional\n",
        "    return dX, dW, db\n",
        "\n",
        "  def _backward_slow(self, dZ):\n",
        "    '''Naive version (with a bunch of for loops) of the backward pass.\n",
        "    Note: Use self.X, self.X_padded, self.W\n",
        "    \n",
        "    :param dZ: Gradient of the subsequent layer.\n",
        "    :returns: A pair (dX, dW and db) which contains the partial derivatives respect to the input and to the parameters (W and b). See the lecture notes for the \"definition\".\n",
        "    '''    \n",
        "    db = None  # None\n",
        "    dW = None  # None\n",
        "    dX = None  # None \n",
        "    return dX, dW, db\n",
        "  \n",
        "  def _backward(self, dZ):\n",
        "    if self.use_fast:\n",
        "      dX, dW, db = self._backward_fast(dZ)\n",
        "    else:\n",
        "      dX, dW, db = self._backward_slow(dZ)\n",
        "    \n",
        "    if 0 < self.pad:\n",
        "      dX = dX[:, self.pad:-self.pad, self.pad:-self.pad, :]\n",
        "    \n",
        "    return dX, [dW, db]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4WS-Qljlt10",
        "colab_type": "text"
      },
      "source": [
        "# Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj-9zrgJlwdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  def __init__(self, layers=None, loss=None, optimizer=None):\n",
        "    self.layers = []\n",
        "    if layers is not None:\n",
        "      self.layers = layers\n",
        "    self.loss = loss\n",
        "    self.optimizer = optimizer\n",
        "  \n",
        "  def add(self, layer):\n",
        "    assert isinstance(layer, Layer)\n",
        "    layer.optimizer = self.optimizer\n",
        "    self.layers.append(layer)\n",
        "  \n",
        "  def train(self, x_train, y_train, n_epochs, batch_size, randomize=True, display=True):\n",
        "    self.losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "      idx_list = list(range(len(x_train)))\n",
        "      if randomize:\n",
        "        np.random.shuffle(idx_list)\n",
        "      n_batches = (len(idx_list) + batch_size - 1) // batch_size\n",
        "      loss = 0.\n",
        "      for batch_idx in range(n_batches):\n",
        "        data = x_train[idx_list[batch_idx * batch_size:(batch_idx + 1) * batch_size]]\n",
        "        # TODO forward pass\n",
        "        \n",
        "        # loss\n",
        "        y_pred = data\n",
        "        y_true = y_train[idx_list[batch_idx * batch_size:(batch_idx + 1) * batch_size]]\n",
        "        batch_loss = self.loss(y_true, y_pred)\n",
        "        loss += batch_loss\n",
        "\n",
        "        # TODO backward pass\n",
        "\n",
        "        # display\n",
        "        # print('Epoch {}/{}: batch {}/{}: batch_loss: {}, avg_loss: {}'.format(epoch+1, n_epochs, batch_idx+1, n_batches, batch_loss, loss/(batch_idx+1)))  #, end='\\r')\n",
        "      print('Epoch {}/{}: loss: {}'.format(epoch+1, n_epochs, loss/n_batches))\n",
        "      self.losses.append(loss / n_batches)\n",
        "    if display:\n",
        "      pass # TODO Plot the learning curve after training.\n",
        "  \n",
        "  def predict(self, x_test, display=True):\n",
        "    pass  # TODO (hint: forward pass)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BzvGvB8mxRc",
        "colab_type": "text"
      },
      "source": [
        "# Breast Cancer Wisconsin (Diagnostic) Dataset\n",
        "\n",
        "For more information, see: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTviPyJjnGGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwcGuHl4nFcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'wdbc.data'\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "  lines = file.readlines()\n",
        "  \n",
        "words = [line.split(',') for line in lines]\n",
        "data = words[:-1]\n",
        "\n",
        "features = [[float(item) for item in rec[2:]] for rec in data]\n",
        "features = np.array(features, dtype=np.float32)\n",
        "\n",
        "label_str_to_num = lambda label: 1. if label == 'M' else 0.\n",
        "labels = [label_str_to_num(rec[1]) for rec in data]\n",
        "labels = np.array(labels, dtype=np.float32)\n",
        "\n",
        "# removing records with missing features (if any feature == 0.)\n",
        "missing_features = np.any(features == 0., axis=1)\n",
        "features = features[~missing_features,:]\n",
        "labels = labels[~missing_features]\n",
        "\n",
        "print(features.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81mWfZDknrC7",
        "colab_type": "text"
      },
      "source": [
        "## Data exploration and Pre-processing\n",
        "Tasks:\n",
        "* Print the distribution of the labels.\n",
        "* Print the scales of each features. (min, max, avg, std)\n",
        "* Randomly split the dataset to training and test sets. (Ratio should be 80-20.)\n",
        "  * After splitting make sure that the distribution of the labels are similar. (Print the distribution of the labels.)\n",
        "* Normalize the data by each feature. (Use Z-score standardization.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aakEtt2crhFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8zz9YlvpGoF",
        "colab_type": "text"
      },
      "source": [
        "## Define and train a model.\n",
        "Tasks:\n",
        "* Define a 2 layer fully-connected network:\n",
        "  1. layer with 5 units and ReLU activation.\n",
        "  2. layer with 1 units and Sigmoid activation.\n",
        "* Train it for 500 epochs with a batch size of 16 using SGD optimizer with learning rate of 0.01.\n",
        "* Plot the learning curve.\n",
        "  * Summmarize in a few words what you see. (Presence of overfitting, underfitting, ...)\n",
        "* Evaluate the trained model on the test set. (Loss, accuracy, precision, recall)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAw51sN0riJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRThipH0rICy",
        "colab_type": "text"
      },
      "source": [
        "# Triangles vs Circles (Basic Shapes Dataset)\n",
        "\n",
        "For more information, see: https://www.kaggle.com/cactus3/basicshapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrk4opOGrnci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1Va88vwMwmToi0SFfdTo8_9TJsV4qdOXK\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1Va88vwMwmToi0SFfdTo8_9TJsV4qdOXK\" -o circles_vs_triangles_data.pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqhCk_tc5ces",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('circles_vs_triangles_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "features, labels = data['features'], data['labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85kkUBgzrlSj",
        "colab_type": "text"
      },
      "source": [
        "## Data exploration and Pre-processing\n",
        "Tasks:\n",
        "* Print the distribution of the labels.\n",
        "* Plot some images (3 circles and 3 triangles).\n",
        "* Randomly split the dataset to training and test sets. (Ratio should be 80-20.)\n",
        "  * After splitting make sure that the distribution of the labels are similar. (Print the distribution of the labels.)\n",
        "* Normalize the data between -1. and 1. (Use min-max scaling.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5CWlDOTsFG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDvYFMjKsGJz",
        "colab_type": "text"
      },
      "source": [
        "## Define and train a model.\n",
        "Tasks:\n",
        "* Define a simple convolutional network as follows: 2x(Conv2D with 4 [then 8 in the second time] (3x3) kernels -> ReLU -> MaxPooling2D) -> Flatten -> Dense with 32 units -> Relu -> Dense -> Sigmoid\n",
        "* Train it for 100 (or less if it is too slow...) epochs with a batch size of 16 using SGD optimizer with learning rate of 0.01.\n",
        "  * Optional: Also train it with the optimized implementation, and measure the speed up. \n",
        "* Plot the learning curve.\n",
        "  * Summmarize in a few words what you see. (Presence of overfitting, underfitting, ...)\n",
        "* Evaluate the trained model on the test set. (Loss, accuracy, precision, recall)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0Ky-8ESsI4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}